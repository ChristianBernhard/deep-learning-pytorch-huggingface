# script parameters
model_id: "meta-llama/Meta-Llama-3.1-405B" # Hugging Face model id
# model_id: "hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16" # Hugging Face model id
dataset_path: "."                      # path to dataset
max_seq_len:  2048                     # max sequence length for model and packing of the dataset
# training parameters
output_dir: "./llama-31-405b-hf-no-robot" # Temporary output directory for model checkpoints
report_to: "tensorboard"               # report metrics to tensorboard
learning_rate: 2.0e-4                  # learning rate 2.0e-4
lr_scheduler_type: "constant"          # learning rate scheduler
num_train_epochs: 3                    # number of training epochs
per_device_train_batch_size: 2         # batch size per device during training
per_device_eval_batch_size: 1          # batch size for evaluation
gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 10                      # log every 10 steps
save_strategy: epoch                   # save checkpoint every epoch
eval_strategy: epoch                   # evaluate every epoch
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.03                     # warmup ratio
bf16: true                             # use bfloat16 precision
tf32: true                             # use tf32 precision
gradient_checkpointing: true          # we use activation_checkpointing instead



