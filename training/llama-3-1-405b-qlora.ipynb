{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Llama 3.1 405B on a single Node with PyTorch FSDP and Q-LoRA.\n",
    "\n",
    "The release of [Llama 3.1 405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B) marks a significant change in the landscape of large language models, setting a new benchmark for performance in general knowledge, reasoning, and multilingual tasks. As Meta's largest open-source model, Llama 3.1 405B competes directly with proprietary models like GPT-4 and Claude 3.5 Sonnet, offering frontier-level capabilities at a more accessible price point.\n",
    "\n",
    "This blog post will guide you through the process of fine-tuning Llama 3.1 405B using PyTorch FSDP and Q-LoRA, supported by Hugging Face's [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index). We will also integrate [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) for enhanced performance.\n",
    "\n",
    "1. **Setup development environment**\n",
    "2. **Create and prepare the dataset**\n",
    "3. **Fine-tune the LLM with PyTorch FSDP, Q-LoRA, and SDPA**\n",
    "4. **Test Model and run Inference**\n",
    "\n",
    "_Note: This example is optimized for NVIDIA H100 and A100 GPUs. Adjustments can be made for different hardware configurations._ \n",
    "\n",
    "**Background on FSDP and Q-LoRA**\n",
    "\n",
    "FSDP enables efficient model sharding across GPUs, allowing the training of large models like Llama 3.1 405B. Q-LoRA reduces computational and memory requirements by combining quantization and low-rank adaptation. This collaboration between [Answer.AI](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html), [Tim Dettmers](https://github.com/TimDettmers/bitsandbytes), and [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "For more details on these techniques, refer to the following resources:\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "%pip install \"torch==2.4.0\" tensorboard\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \"transformers==4.44.0\" \"datasets==2.21.0\" \"accelerate==0.34.2\" \"evaluate==0.4.3\" \"bitsandbytes==0.43.3\" \"huggingface_hub==0.24.7\" \"trl==0.10.1\" \"peft==0.12.0\" \"hf_transfer==0.1.8\" \"flash-attn==2.6.3\"\n",
    "# if your are running on AWS cluster you might need to update the nccl version\n",
    "%pip install nvidia-nccl-cu12==2.22.3 --upgrade\n",
    "\n",
    "# Install transformers from git commit to support pre-quantized weights\n",
    "%pip install git+https://github.com/huggingface/transformers.git@18e1a9c7195543ec7b7314fcd995bc7aad559e66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: tensorboard in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (2.17.1)\n",
      "Requirement already satisfied: filelock in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.20)\n",
      "Requirement already satisfied: absl-py>=0.4 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (1.65.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (5.27.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tensorboard) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Installing collected packages: nvidia-nccl-cu12\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
      "Successfully installed nvidia-nccl-cu12-2.20.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.44.0\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets==2.21.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (2.21.0)\n",
      "Collecting accelerate==0.33.0\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting evaluate==0.4.1\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: bitsandbytes==0.43.3 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (0.43.3)\n",
      "Collecting huggingface_hub==0.24.2\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl==0.9.6\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: peft==0.12.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: hf_transfer==0.1.8 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (0.1.8)\n",
      "Requirement already satisfied: flash-attn==2.6.3 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (2.6.3)\n",
      "Requirement already satisfied: filelock in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.44.0) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from datasets==2.21.0) (3.10.5)\n",
      "Requirement already satisfied: psutil in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from accelerate==0.33.0) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from accelerate==0.33.0) (2.4.0)\n",
      "Requirement already satisfied: responses<0.19 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from huggingface_hub==0.24.2) (4.12.2)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from trl==0.9.6) (0.8.8)\n",
      "Requirement already satisfied: einops in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from flash-attn==2.6.3) (0.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from aiohttp->datasets==2.21.0) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.44.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.44.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.44.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.44.0) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.33.0) (12.6.20)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.9.6) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.9.6) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from pandas->datasets==2.21.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
      "Using cached transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Downloading huggingface_hub-0.24.2-py3-none-any.whl (417 kB)\n",
      "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: huggingface_hub, transformers, accelerate, trl, evaluate\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.0.dev0\n",
      "    Uninstalling transformers-4.45.0.dev0:\n",
      "      Successfully uninstalled transformers-4.45.0.dev0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.2\n",
      "    Uninstalling accelerate-0.34.2:\n",
      "      Successfully uninstalled accelerate-0.34.2\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.10.1\n",
      "    Uninstalling trl-0.10.1:\n",
      "      Successfully uninstalled trl-0.10.1\n",
      "  Attempting uninstall: evaluate\n",
      "    Found existing installation: evaluate 0.4.3\n",
      "    Uninstalling evaluate-0.4.3:\n",
      "      Successfully uninstalled evaluate-0.4.3\n",
      "Successfully installed accelerate-0.33.0 evaluate-0.4.1 huggingface_hub-0.24.2 transformers-4.44.0 trl-0.9.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nvidia-nccl-cu12==2.22.3\n",
      "  Using cached nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Using cached nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\n",
      "Installing collected packages: nvidia-nccl-cu12\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.4.0 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.22.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/huggingface/transformers.git@18e1a9c7195543ec7b7314fcd995bc7aad559e66\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 18e1a9c7195543ec7b7314fcd995bc7aad559e66) to /tmp/pip-req-build-mjojl4tv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-mjojl4tv\n",
      "  Running command git rev-parse -q --verify 'sha^18e1a9c7195543ec7b7314fcd995bc7aad559e66'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 18e1a9c7195543ec7b7314fcd995bc7aad559e66\n",
      "  Running command git checkout -q 18e1a9c7195543ec7b7314fcd995bc7aad559e66\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 18e1a9c7195543ec7b7314fcd995bc7aad559e66\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.45.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.45.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fsx/philipp/conda/envs/dev/lib/python3.11/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9734656 sha256=b797ae19c3821b6bf8f135c4cbaa1dd58585dc0fcfef68e6fe61f01a11e5f280\n",
      "  Stored in directory: /fsx/philipp/.cache/wheels/72/02/99/fa1efd615b475db3c610a32466ff59f5d48c3c25d422a14f46\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.0\n",
      "    Uninstalling transformers-4.44.0:\n",
      "      Successfully uninstalled transformers-4.44.0\n",
      "Successfully installed transformers-4.45.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "%pip install \"torch==2.4.0\" tensorboard\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \"transformers==4.44.0\" \"datasets==2.21.0\" \"accelerate==0.33.0\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.3\" \"huggingface_hub==0.24.2\" \"trl==0.9.6\" \"peft==0.12.0\" \"hf_transfer==0.1.8\" \"flash-attn==2.6.3\"\n",
    "# if your are running on AWS cluster you might need to update the nccl version\n",
    "%pip install nvidia-nccl-cu12==2.22.3 --upgrade\n",
    "\n",
    "# Install transformers from git commit to support pre-quantized weights\n",
    "%pip install git+https://github.com/huggingface/transformers.git@18e1a9c7195543ec7b7314fcd995bc7aad559e66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to login into Hugging Face to access the Llama 3.1 405b model. If you don't have an account yet and [accepted the terms](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B), you can create one [here](https://huggingface.co/join). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "After our environment is set up, we can start creating and preparing our dataset. A fine-tuning dataset should have a diverse set of demonstrations of the task you want to solve. If you want to learn more about how to create a dataset, take a look at the [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset).\n",
    "\n",
    "We will use the [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. This data can be used for supervised fine-tuning (SFT) to make language models follow instructions better. No Robots was modelled after the instruction dataset described in OpenAI's [InstructGPT paper](https://huggingface.co/papers/2203.02155), and is comprised mostly of single-turn instructions.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "The [no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset has 10,000 split into 9,500 training and  500 test examples. Some samples are not including a `system` message. We will load the dataset with the `datasets` library, add a missing `system` message and save them to separate json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63c07f210804bed9a47eccd53665489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ec71d615947f2b21fd1793eb25bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a53adfafc4456ebc830f3f6df6ad27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/571k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc717b48a49e4cd99d9bf4007d7355dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da19a081baa24d3b8ea7d0738310c923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33816f942294736ad561a3993e885e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043f48108174ac6a49a114c62a9f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68f2c0422a4c6788264379956b734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741d77a3067e45caa1d7195155914655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00dd00b336d4b55868904a57c9d4ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b503c24904504268b45fce9180d8b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "784047"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "\n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 405B with PyTorch FSDP, Q-Lora and SDPA\n",
    "\n",
    "We are now ready to fine-tune our model with PyTorch FSDP, Q-Lora and SDPA. Since we are running in a distributed setup, we need to use `torchrun` and a python script to start the training. \n",
    "\n",
    "We prepared a script [run_fsdp_qlora.py](./scripts/run_fsdp_qlora.py) which will load the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs supporting:\n",
    "* Dataset formatting, including conversational and instruction format (✅ used)\n",
    "* Training on completions only, ignoring prompts (❌ not used)\n",
    "* Packing datasets for more efficient training (✅ used)\n",
    "* PEFT (parameter-efficient fine-tuning) support including Q-LoRA (✅ used)\n",
    "* Preparing the model and tokenizer for conversational fine-tuning (❌ not used, see below)\n",
    "\n",
    "_Note: We are using an Anthropic/Vicuna like Chat Template with `User:` and `Assistant:` roles. This done because the special tokens in base Llama 3 (`<|begin_of_text|>` or `<|reserved_special_token_XX|>`) are not trained. Meaning if want would like to use them for the template we need to train them which requires more memory, since we need to update the embedding layer and lm_head. If you have access to more compute you can modify `LLAMA_3_CHAT_TEMPLATE` in the [run_fsdp_qlora.py](./scripts/run_fsdp_qlora.py) script._\n",
    "\n",
    "For configuration we use the new `TrlParser`, that allows us to provide hyperparameters in a yaml file or overwrite the arguments from the config file by explicitly passing them to the CLI, e.g. `--num_epochs 10`.\n",
    "\n",
    "_Note: The config below is optimized for 8x H100 80GBs, you can also fine-tune Llama 3.1 405B on 4x H100. Therefore change the `per_device_train_batch_size` to `2`. With a `max_seq_len` of `2048` this should lead to ~64GB per GPU._ \n",
    "\n",
    "**Pre-quantize Llama 3.1 405B**\n",
    "\n",
    "We are using the [pre-quantized version of Llama 3.1 405B](hugging-quants/Meta-Llama-3.1-70B-BNB-NF4-BF16), which is already pre-quantized using `bitsandbytes`. When you fine-tune a model with Q-LoRA thats not pre-quantized `bitsandbytes` will first quantize the model and then start the training process. By using the pre-quantized model we can save time as the model is significant smaller to download and to load on the GPU. You can learn more about pre-quantizing models with `bitsandbytes` in the [bitsandbytes documentation](). \n",
    "\n",
    "\n",
    "**Tested Hardware Configurations:**  \n",
    "✅ 4x H100 80GB with ~900GB CPU RAM  \n",
    "✅ 8x H100 80GB with ~1.5TB CPU RAM  \n",
    "❌ 8x L40 48GB _(might work but not tested)._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama_31_405b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_31_405b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "# model_id: \"meta-llama/Meta-Llama-3.1-405B\" # Hugging Face model id\n",
    "model_id: \"hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16\" # Hugging Face model id\n",
    "dataset_path: \".\"                      # path to dataset\n",
    "max_seq_len:  2048                     # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-31-405b-hf-no-robot\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 2.0e-4                  # learning rate 2.0e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 2         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 4         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "eval_strategy: epoch                   # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # we use activation_checkpointing instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: At the end of the training there will be a slight increase in GPU memory usage (~10%). This is due to the saving of the model correctly. Make sure to have enough memory left on your GPU to save the model. [REF](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp#memory-usage)_\n",
    "\n",
    "We are going to use accelerate to distribute the training across multiple GPUs. Accelerate is a PyTorch library that makes it easier to write distributed PyTorch code. It provides a high-level API that abstracts away the complexity of distributed training. We created a [fsdp_qlora.yaml](./configs/fsdp_qlora.yaml) configuration file that contains the environment configuration for the training. Here you can change the number of GPUs (`num_processes`) or FSDP configuration. \n",
    "\n",
    "Now, lets launch the training with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "## Run training via slurm script slurm | script | accelerate | config\n",
    "# sbatch --job-name=l31-405 slurm/hf.slurm scripts/run_fsdp_qlora.py configs/fsdp_qlora.yaml llama_31_405b_fsdp_qlora.yaml\n",
    "# Run training via terminal\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 accelerate launch --config_file ./configs/fsdp_qlora.yaml --num_processes 8 ./scripts/run_fsdp_qlora.py --config llama_31_405b_fsdp_qlora.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of Llama 3.1 405B with Flash Attention for 3 epochs with a dataset of 10k samples takes ~3h on a `p5.48xlarge` (8x H100). The on-demand instance price is `$98.320/h` which would result in a total cost of `~300$`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Merge LoRA adapter in to the original model_\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference.\n",
    "\n",
    "Since we used the pre-quantized model, we need to load the full-precision model and then merge the adapter weights into the model weights. \n",
    "\n",
    "_Note: You might require > 900GB CPU Memory and it takes ~90min (including downloading, loading and merging)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d4ec9b3d54f5fa18fe8211d764c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.32.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.33.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.34.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.35.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.36.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.37.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.38.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.39.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.40.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.41.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.42.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.43.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.44.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.45.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.46.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.47.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.48.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.49.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.50.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.51.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.52.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.53.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.54.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.55.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.56.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.57.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.58.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.59.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.60.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.60.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.61.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.61.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.62.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.62.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.63.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.63.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.64.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.64.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.65.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.65.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.66.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.66.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.67.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.67.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.68.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.68.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.69.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.69.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.70.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.70.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.71.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.71.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.72.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.72.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.73.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.73.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.74.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.74.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.75.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.75.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.76.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.76.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.77.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.77.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.78.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.78.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.79.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.79.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.80.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.80.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.81.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.81.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.82.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.82.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.83.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.83.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.84.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.84.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.85.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.85.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.86.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.86.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.87.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.87.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.88.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.88.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.89.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.89.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.90.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.90.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.91.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.91.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.92.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.92.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.93.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.93.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.94.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.94.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.95.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.95.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.96.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.96.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.97.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.97.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.98.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.98.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.99.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.99.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.100.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.100.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.101.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.101.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.102.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.102.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.103.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.103.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.104.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.104.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.105.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.105.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.106.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.106.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.107.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.107.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.108.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.108.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.109.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.109.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.110.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.110.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.111.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.111.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.112.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.112.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.113.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.113.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.114.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.114.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.115.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.115.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.116.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.116.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.117.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.117.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.118.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.118.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.119.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.119.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.120.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.120.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.121.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.121.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.122.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.122.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.123.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.123.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.124.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.124.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.125.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.125.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     base_model_path,\n\u001b[1;32m     13\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     14\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )  \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load loRA Adapter\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Merge LoRA and base model and save\u001b[39;00m\n\u001b[1;32m     20\u001b[0m merged_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m/fsx/philipp/conda/envs/dev/lib/python3.11/site-packages/transformers/integrations/peft.py:214\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     processed_adapter_state_dict[new_key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_adapter_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incompatible_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# check only for unexpected keys\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(incompatible_keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(incompatible_keys\u001b[38;5;241m.\u001b[39munexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/fsx/philipp/conda/envs/dev/lib/python3.11/site-packages/peft/utils/save_and_load.py:395\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    392\u001b[0m peft_model_state_dict, mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m    393\u001b[0m     model, peft_model_state_dict, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes\n\u001b[1;32m    394\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    397\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    398\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m/fsx/philipp/conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.32.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.32.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.33.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.33.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.34.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.34.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.35.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.35.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.36.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.36.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.37.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.37.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.38.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.38.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.39.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.39.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.40.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.40.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.41.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.41.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.42.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.42.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.43.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.43.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.44.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.44.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.45.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.45.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.46.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.46.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.47.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.47.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.48.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.48.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.49.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.49.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.50.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.50.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.51.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.51.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.52.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.52.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.53.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.53.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.54.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.54.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.55.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.55.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.56.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.56.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.57.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.57.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.58.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.58.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.59.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.59.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.60.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.60.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.61.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.61.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.62.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.62.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.63.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.63.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.64.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.64.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.65.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.65.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.66.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.66.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.67.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.67.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.68.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.68.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.69.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.69.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.70.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.70.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.71.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.71.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.72.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.72.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.73.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.73.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.74.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.74.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.75.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.75.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.76.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.76.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.77.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.77.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.78.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.78.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.79.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.79.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.80.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.80.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.81.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.81.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.82.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.82.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.83.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.83.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.84.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.84.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.85.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.85.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.86.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.86.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.87.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.87.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.88.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.88.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.89.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.89.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.90.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.90.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.91.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.91.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.92.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.92.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.93.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.93.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.94.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.94.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.95.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.95.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.96.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.96.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.97.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.97.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.98.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.98.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.99.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.99.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.100.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.100.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.101.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.101.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.102.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.102.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.103.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.103.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.104.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.104.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.105.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.105.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.106.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.106.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.107.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.107.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.108.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.108.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.109.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.109.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.110.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.110.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.111.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.111.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.112.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.112.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.113.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.113.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.114.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.114.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.115.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.115.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.116.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.116.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.117.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.117.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.118.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.118.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.119.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.119.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.120.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.120.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.121.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.121.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.122.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.122.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.123.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.123.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.124.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.124.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.125.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16]).\n\tsize mismatch for model.layers.125.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([2048, 16]) from checkpoint, the shape in current model is torch.Size([1024, 16])."
     ]
    }
   ],
   "source": [
    "### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model paths\n",
    "base_model_path = \"meta-llama/Meta-Llama-3.1-405B\"\n",
    "peft_model_path = \"./llama-31-405b-hf-no-robot\"\n",
    "save_model_path = \"./llama-31-405b-hf-no-robot-merged\"\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Load loRA Adapter\n",
    "model.load_adapter(peft_model_path)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(save_model_path,safe_serialization=True, max_shard_size=\"5GB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model manually. Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) or [LLM Evaluation doesn't need to be complicated](https://www.philschmid.de/llm-evaluation) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b838953dfad0438f9d1f42c712376973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "peft_model_id = \"./llama-31-405b-hf-no-robot\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config={\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load our test dataset try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "We want to teach our daughter the value of a clean home. Can you suggest chores that are appropriate for a child who is who 5 years old?\n",
      "\n",
      "**Original Answer:**\n",
      "No Problem! Here are some great chores for a 5-year-old to do. \n",
      "\n",
      "     • Vacuuming: Most kids love the vacuum so this is a great starter chore for a five-year-old. Make sure to go through the do’s and don’t of vacuuming and remind your child not everything gets picked up by a \n",
      "       vacuum. \n",
      "     • Bedroom Cleanup: Teach your child to take care of their bedrooms. Start with picking up toys, this can be made into a game by color or items. Another good way of teaching children to fine-tune their bedrooms is \n",
      "       lining up their shoes, making their beds, and ensuring nothing is out of order. \n",
      "     • Separate Laundry: At 5 a children can easily distinguish between colors, whites, and towels. Have your children separate the laundry to make the task of washing clothes easier and more efficient. \n",
      "     • Folding Laundry: This is a great time to teach children how to fold laundry. At 5 they won’t be perfect but little hands can handle washcloths, hand towels, and mating socks. \n",
      "     • Putting Away Groceries: This is a good way for children to learn everything has a place to go. With your guidance, a small task like putting away groceries will teach them later on how to put away and keep things \n",
      "       neat. \n",
      "     • Dusting: Grab a duster and teach your child how to carefully remove things to dust tables, handrails, or other kid-friendly dustable areas. \n",
      "     • Set and Clean The Table: Most households with small kids have kid-friendly items for the table. Allow your child to set the table before dinner.  After dinner teach your child to remove their items and take them to \n",
      "        the kitchen sink to be rinsed off. This chore will become a healthy habit. \n",
      "     • Make The Bed: At five this won’t be perfect, but a child will be able to learn to set the pillow properly and adjust their bedding so it's more tidy. If they use extra blankets at night teach them to put them away or \n",
      "       fold them at the bottom of the bed. Don’t forget to adjust the stuffed animals!\n",
      "\n",
      "\n",
      "**Generated Answer:**\n",
      "1. Making the bed. This is a simple task that helps to teach your daughter the importance of starting the day with a clean and organized space.\n",
      "2. Picking up toys. This is a great way to teach your daughter the importance of putting things away when she is finished using them.\n",
      "3. Putting away clothes. This is a good way to teach your daughter the importance of keeping her clothes organized and clean.\n",
      "4. Dusting. This is a simple task that can help to keep your home clean and dust-free.\n",
      "5. Vacuuming. This is a great way to teach your daughter the importance of keeping her home clean and free of dirt and debris.\n",
      "6. Wiping down surfaces. This is a good way to teach your daughter the importance of keeping her home clean and free of germs.\n",
      "7. Sweeping. This is a simple task that can help to keep your home clean and free of dirt and debris.\n",
      "8. Mopping. This is a great way to teach your daughter the importance of keeping her home clean and free of dirt and debris.\n",
      "9. Taking out the trash. This is a good way to teach your daughter the importance of keeping her home clean and free of garbage.\n",
      "10. Watering plants. This is a simple task that can help to teach your daughter the importance of caring for plants.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "messages = eval_dataset[rand_idx][\"messages\"][:2]\n",
    "\n",
    "# Test on sample \n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True).strip()}\")\n",
    "\n",
    "# **Query:**\n",
    "# How long was the Revolutionary War?\n",
    "# **Original Answer:**\n",
    "# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783. \n",
    "# **Generated Answer:**\n",
    "# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! 🚀 Now, its your turn! \n",
    "\n",
    "If you want to deploy your model into production check out [Deploy the LLM for Production](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#6-deploy-the-llm-for-production)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
